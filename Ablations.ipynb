{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AgKq9-spa5u",
        "outputId": "6bebb6e7-53f3-4f13-dd12-41bd2a3bae3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import alsh_conv_2d as Conv\n",
        "from multi_hash_srp import MultiHash_SRP"
      ],
      "metadata": {
        "id": "tToz42gCvvMa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9GSzKLovscl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "FTHJlSEewm7C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 64)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PwvEs0O4wq1v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ALSHCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ALSHCNN, self).__init__()\n",
        "    self.conv1 = Conv.ALSHConv2d(1, 32, 3, 1, 1, 1, True, MultiHash_SRP, {}, 5, 8, 2**5, 16)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    #self.conv2 = Conv.ALSHConv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = Conv.ALSHConv2d(32, 64, 3, 1, 1, 1, True, MultiHash_SRP, {}, 5, 8, 2**5, 16)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv3 = Conv.ALSHConv2d(64, 64, 3, 1, 1, 1, True, MultiHash_SRP, {}, 5, 8, 2**5, 16)\n",
        "    #self.conv3 = Conv.ALSHConv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu3 = nn.ReLU()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 64)\n",
        "    self.relu4 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "        print(x.shape)\n",
        "\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z7-8tjoc6QWc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_ALSH_mode(m):\n",
        "    if isinstance(m, Conv.ALSHConv2d):\n",
        "        m.ALSH_mode()"
      ],
      "metadata": {
        "id": "ix9NnnqY-b3n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Step 2: Define the neural network model (AlexNet)\n",
        "model = models.alexnet(pretrained=True)  # Assuming CIFAR-10 has 10 classes\n",
        "ftr_weight = model.features[-1].weight\n",
        "r = Conv.ALSHConv2d(64, 256, 3, 1, 1, 1, True, MultiHash_SRP, {}, 5, 8, 2**5)\n",
        "r.weight = ftr_weight\n",
        "model.features[-1] = r\n",
        "\n",
        "ftr_weight = model.features[-2].weight\n",
        "r = Conv.ALSHConv2d(64, 256, 3, 1, 1, 1, True, MultiHash_SRP, {}, 5, 8, 2**5)\n",
        "r.weight = ftr_weight\n",
        "model.features[-2] = r\n",
        "\n",
        "model.apply(set_ALSH_mode)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Step 4: Choose an optimization algorithm\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Step 5: Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 5  # You can adjust this based on your requirements\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Training time: {elapsed_time:.2f} seconds')\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "BA8MueAG6mYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth = 0"
      ],
      "metadata": {
        "id": "YELOrNesJACJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_next_conv(model, current):\n",
        "    while not isinstance(model.features[current], nn.Conv2d):\n",
        "        current -= 1\n",
        "    if isinstance(model.features[current], nn.Conv2d):\n",
        "        print('REPLACED REGULAR Conv2d WITH ALSHConv2d')\n",
        "        model.features[current] = Conv.ALSHConv2d.build(\n",
        "            model.features[current], MultiHash_SRP, {}, 5, 3, 2**5, 64)\n",
        "    return current-1"
      ],
      "metadata": {
        "id": "8IUf2JykIr21"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = Conv.ALSHConv2d(64, 256, 3, 1, 1, 1, True, MultiHash_SRP, {}, 5, 8, 2**5, 64)\n",
        "model.features[-1] = r"
      ],
      "metadata": {
        "id": "Ju5gLXSN6-BQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LTEZgSSAjQu",
        "outputId": "1f6dd3db-a7d3-46ac-a611-ec89490b1ac0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOxQPsouAJA0",
        "outputId": "92392e1e-0e69-41ed-dc95-3f132197cda6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 117688868.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 31961924.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25176619.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14142931.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "# model = ALSHCNN()\n",
        "# depth = 3\n",
        "# current = 0\n",
        "# for d in range(depth):\n",
        "#         replace_next_conv(model.features[current])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "epochs = 5\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Training time: {elapsed_time:.2f} seconds')\n",
        "# Test the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "iUJKPOlIpuHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Model, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1),\n",
        "            nn.Hardshrink(lambd=0.3),\n",
        "            #nn.Softshrink(),\n",
        "            #nn.ReLU(),\n",
        "            #nn.Tanh(),\n",
        "            #nn.ELU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "            nn.Hardshrink(lambd=0.3),\n",
        "            #nn.Softshrink(),\n",
        "            #nn.ReLU(),\n",
        "            #nn.Tanh(),\n",
        "            #nn.ELU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.classifier = nn.Sequential(nn.Dropout(),\n",
        "                                        nn.Linear(3872, num_classes),\n",
        "                                        nn.Softmax(dim=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "5mYZyd7E_LrO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * 56*56, 128),  # Adjust input size based on your input dimensions\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.reshape(16, 32*56*56)  # Adjust view size based on your input dimensions\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleCNN(num_classes=10)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Y-LF4cE8lU",
        "outputId": "d388f086-cb0e-4833-b6ee-ef78fc31a527"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=100352, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=16, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix(m):\n",
        "    if isinstance(m, Conv.ALSHConv2d):\n",
        "        m.fix()\n",
        "        m.cuda()"
      ],
      "metadata": {
        "id": "_ELzHltAAjqt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = 0.001\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "QWmKfWu6BXXn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kON51iDjIV5w"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.val:.3f})\\t'.format(\n",
        "                      epoch,\n",
        "                      i,\n",
        "                      len(train_loader),\n",
        "                      data_time=data_time,\n",
        "                      loss=loss.item(),\n",
        "                      ))\n",
        "            print(\"Loss:\" + str(loss.item()))"
      ],
      "metadata": {
        "id": "CvtmvDGmBlEZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler = None\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=16,\n",
        "                                               shuffle=(train_sampler is None),\n",
        "                                               num_workers=2,\n",
        "                                               pin_memory=True,\n",
        "                                               sampler=train_sampler)"
      ],
      "metadata": {
        "id": "1SRVU37_AyNh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LH-RvxUeiBtP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN()\n",
        "model = model.cuda()\n",
        "\n",
        "for i in range(depth):\n",
        "  replace_next_conv(model, depth - i - 1)"
      ],
      "metadata": {
        "id": "6baAGdfHITjX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), 0.001)\n",
        "\n",
        "current_depth = len(model.features) - 1\n",
        "flag = True\n",
        "start_time = time.time()\n",
        "#replace_next_conv(model, current_depth)\n",
        "for epoch in range(5):\n",
        "    train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "end_time = time.time()\n",
        "print(end_time - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wjt6r2SAXa7",
        "outputId": "fd17a6df-e295-4317-a871-40d3b6583400"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/3125]\tData 0.078 (0.078)\t\n",
            "Loss:2.789382219314575\n",
            "Epoch: [0][1000/3125]\tData 0.020 (0.020)\t\n",
            "Loss:1.0489041805267334\n",
            "Epoch: [0][2000/3125]\tData 0.019 (0.019)\t\n",
            "Loss:1.5077332258224487\n",
            "Epoch: [0][3000/3125]\tData 0.021 (0.021)\t\n",
            "Loss:1.534446120262146\n",
            "Epoch: [1][0/3125]\tData 0.072 (0.072)\t\n",
            "Loss:1.3543429374694824\n",
            "Epoch: [1][1000/3125]\tData 0.020 (0.020)\t\n",
            "Loss:1.2730414867401123\n",
            "Epoch: [1][2000/3125]\tData 0.012 (0.012)\t\n",
            "Loss:1.3833974599838257\n",
            "Epoch: [1][3000/3125]\tData 0.000 (0.000)\t\n",
            "Loss:0.9389098882675171\n",
            "Epoch: [2][0/3125]\tData 0.073 (0.073)\t\n",
            "Loss:0.7708405256271362\n",
            "Epoch: [2][1000/3125]\tData 0.008 (0.008)\t\n",
            "Loss:1.1802189350128174\n",
            "Epoch: [2][2000/3125]\tData 0.000 (0.000)\t\n",
            "Loss:1.2445745468139648\n",
            "Epoch: [2][3000/3125]\tData 0.000 (0.000)\t\n",
            "Loss:1.2946547269821167\n",
            "Epoch: [3][0/3125]\tData 0.076 (0.076)\t\n",
            "Loss:1.0959255695343018\n",
            "Epoch: [3][1000/3125]\tData 0.000 (0.000)\t\n",
            "Loss:1.117733359336853\n",
            "Epoch: [3][2000/3125]\tData 0.017 (0.017)\t\n",
            "Loss:1.2720227241516113\n",
            "Epoch: [3][3000/3125]\tData 0.014 (0.014)\t\n",
            "Loss:1.1422410011291504\n",
            "Epoch: [4][0/3125]\tData 0.072 (0.072)\t\n",
            "Loss:1.5302592515945435\n",
            "Epoch: [4][1000/3125]\tData 0.000 (0.000)\t\n",
            "Loss:1.0729566812515259\n",
            "Epoch: [4][2000/3125]\tData 0.000 (0.000)\t\n",
            "Loss:0.6196974515914917\n",
            "Epoch: [4][3000/3125]\tData 0.001 (0.001)\t\n",
            "Loss:0.9829186201095581\n",
            "207.33866667747498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "rsSRMRS5BNDI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-iAH1G60BDq6"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}